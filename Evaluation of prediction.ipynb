{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "d658ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "a57452e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class evaluation:\n",
    " \n",
    "    #A constructor that takes two parameters: a list of the predicted classes, and a list of the ground truth.\n",
    "    def __init__(self,ground_truth,prediction):\n",
    "        self.ground_truth = ground_truth\n",
    "        self.prediction = prediction\n",
    "        self.classes = np.unique(self.ground_truth) # no of different classes\n",
    "        self.matrix = np.zeros((len(self.classes), len(self.classes)))\n",
    "        self.precision =np.zeros(len(self.classes))\n",
    "        self.precision=[]\n",
    "        self.recall=[]\n",
    "        self.accuracy=[]\n",
    "        self.f1_score=[]\n",
    "    \n",
    "    #building the confusion matrix\n",
    "    def confusion_matrix(self):\n",
    "         # initialize the confusion matrix with zeros\n",
    "        for i in range(len(self.classes)):\n",
    "            for j in range(len(self.classes)):\n",
    "                self.matrix[i, j] = np.sum((self.ground_truth == self.classes[i]) & (self.prediction == self.classes[j]))\n",
    "        return self.matrix\n",
    "        \n",
    "    #\"getPrecision\": a function that returns the precision of the given predicted and ground truth lists. \n",
    "    def getPrecision(self):\n",
    "        self.precision=[]\n",
    "        for i in range(len(self.classes)):\n",
    "            tp=self.matrix[i][i]\n",
    "            denominator=0\n",
    "            for j in range(len(self.classes)):\n",
    "                denominator=denominator+self.matrix[j][i]\n",
    "            self.precision.append(tp/denominator)\n",
    "        self.precision.append(np.mean(self.precision))\n",
    "        self.precision.append(np.sum(np.diag(self.matrix))/np.sum(self.matrix[:][:]))\n",
    "        return self.precision\n",
    "        \n",
    "        \n",
    "    #\"getRecall\": a function that returns the recall of the given predicted and ground truth lists. \n",
    "    def getRecall(self):\n",
    "        self.recall=[]\n",
    "        for i in range(len(self.classes)):\n",
    "            tp=self.matrix[i][i]\n",
    "            denominator=0\n",
    "            for j in range(len(self.classes)):\n",
    "                denominator=denominator+self.matrix[i][j]\n",
    "            self.recall.append(tp/denominator)\n",
    "        self.recall.append(np.mean(self.recall))\n",
    "        self.recall.append(np.sum(np.diag(self.matrix))/np.sum(self.matrix[:][:]))\n",
    "        return self.recall\n",
    "        \n",
    "    \n",
    "    #\"getAccuracy\": a function that returns the accuracy of the given predicted and ground truth lists.\n",
    "    def getAccuracy(self):\n",
    "        self.accuracy=[]\n",
    "        for i in range(len(self.classes)):\n",
    "            tp=self.matrix[i][i]\n",
    "            denominator=0\n",
    "            for j in range(len(self.classes)):\n",
    "                denominator=denominator+self.matrix[i][j]\n",
    "            self.accuracy.append(tp/denominator)\n",
    "        self.accuracy.append(np.mean(self.accuracy))\n",
    "        self.accuracy.append(None)\n",
    "        return self.accuracy\n",
    "        \n",
    "    #\"getF1Score\": a function that returns the F1-score of the given predicted and ground truth lists.\n",
    "    def getF1Score(self):\n",
    "        self.f1_score=[]\n",
    "        for i in range(len(self.classes)):\n",
    "            self.f1_score.append((2*self.precision[i]*self.recall[i])/(self.precision[i]+self.recall[i]))\n",
    "        self.f1_score.append(np.mean(self.f1_score))\n",
    "        tp=np.sum(np.diag(self.matrix))\n",
    "        fp=0\n",
    "        fn=0\n",
    "        for i in range(len(self.classes)):\n",
    "            for j in range(len(self.classes)):\n",
    "                fp=fp+self.matrix[j][i]\n",
    "                fn=fn+self.matrix[i][j]\n",
    "        self.f1_score.append(tp/(tp+(1/2*(fp+fn))))\n",
    "        return self.f1_score\n",
    "        \n",
    "    #\"printEvaluation\": a function that visualizes all the evaluation metrics calculated in a form of a table.\n",
    "    def printEvaluation(self):\n",
    "        self.confusion_matrix()\n",
    "        index=list(self.classes)\n",
    "        index=index+['Macro','Micro']\n",
    "        df = pd.DataFrame(list(zip(self.getPrecision(), self.getRecall(),self.getF1Score(),self.getAccuracy())),\n",
    "                columns =['Precision', 'Recall','F1Score','Accuracy'],\n",
    "                index=index)\n",
    "        print(df)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "29d68a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating object of the class\n",
    "actual_values = [1]*30 + [2]*30 + [3]*30 + [4]*10 # 30 samples of each class 1,2, and 3 and 10 samples of class 4\n",
    "predicted_values = random.choices([1,2,3,4], k=100) # 100 random samples\n",
    "obj = evaluation(actual_values,predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "71ebcc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7., 11.,  5.,  7.],\n",
       "       [ 7.,  4., 11.,  8.],\n",
       "       [ 5.,  8.,  9.,  8.],\n",
       "       [ 1.,  4.,  3.,  2.]])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "6bb95d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Precision    Recall   F1Score  Accuracy\n",
      "1       0.350000  0.233333  0.280000  0.233333\n",
      "2       0.148148  0.133333  0.140351  0.133333\n",
      "3       0.321429  0.300000  0.310345  0.300000\n",
      "4       0.080000  0.200000  0.114286  0.200000\n",
      "Macro   0.224894  0.216667  0.211245  0.216667\n",
      "Micro   0.220000  0.220000  0.180328       NaN\n"
     ]
    }
   ],
   "source": [
    "obj.printEvaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b7e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
